{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWEunswqrzWp"
      },
      "source": [
        "# Exercise 2\n",
        "Due date: 2022-02-09\n",
        "\n",
        "File name convention: For group 42 and memebers Richard Stallman and Linus Torvalds it would be \n",
        "\"02_Exercise2_Goup42_Stallman_Torvalds.pdf\".\n",
        "\n",
        "Submission via blackboard (UA) or Google Form (see notion, LPC).\n",
        "\n",
        "Feel free to answer free text questions in text cells using markdown and possibly $\\LaTeX{}$ if you want to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCY4GDz7rzWw"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/UAPH4582/PH482_582_Sp22/blob/main/Exercises/02_Training_linear_models.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRSpWkx0rzWx"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrId8lV7rzWx"
      },
      "source": [
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZytlnYYrzWy"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c5b04e9rzW0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "HGEkqfDPrzW2",
        "outputId": "aca53379-7992-44de-e7dd-c4ac905eba37"
      },
      "outputs": [],
      "source": [
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tasks 1-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PRAKeQ2rzW3"
      },
      "source": [
        "### Task 1\n",
        "Fit a linear [regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) `lin_reg` and print out its intercept and coefficient. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEupw4u4rzW4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzACHvMk2hd6"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZEguB-ArzW5"
      },
      "outputs": [],
      "source": [
        "# lin_reg = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCHZcxPr2tqY"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FptcwTMhrzW5"
      },
      "source": [
        "### Task 2 \n",
        "With the model you created above, [predict](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict) the new values `X_new`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEYoEbYRrzW6"
      },
      "outputs": [],
      "source": [
        "X_new = np.array([[0], [2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjoh3xeA36Rc"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22Frl1mtrzW7"
      },
      "outputs": [],
      "source": [
        "# y_predict = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9UKK78g38bM"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5v1vgjirzW7"
      },
      "source": [
        "Let's plot the data (blue) and our predictions (red)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "OV7ZcG7PrzW8",
        "outputId": "a3aa8705-fb5d-43fc-c3aa-d7e287ce07ca",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.plot(X, y, \"b.\")\n",
        "plt.plot(X_new, y_predict, \"r-\", markersize = 20)\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq0SLqyyrzW9"
      },
      "source": [
        "### Task 3\n",
        "The `LinearRegression` class is based on the `scipy.linalg.lstsq()` function (the name stands for \"least squares\").\n",
        "\n",
        "Basically we fit to the form $y = 1 \\Theta_0 + \\Theta_1 x = x_b\\cdot \\Theta$ with $x_b = (1, x)^T$ and $\\Theta = (\\Theta_0, \\Theta_1)^T$. So we need to add a 1 to each instance of `X`. See slide 13 of Lecture 4.\n",
        "\n",
        "Look up [`np.linalg.lstsq()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html), call it directly on `X_b`, `y` and print out the $\\Theta$ you found this way. Compare the output with the output from your `LinearRegression` model above. Set `rcond=1e-6` in `np.linalg.lstsq`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pulqnD5VrzW9"
      },
      "outputs": [],
      "source": [
        "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5IeVa0P6k0E"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jphg9m57kvE"
      },
      "outputs": [],
      "source": [
        "# theta_best_svd, residuals, rank, s = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o9Bq2js73A4"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG9puAEZ6anz",
        "outputId": "f473a745-e1c4-4a62-d3dd-1a11a446701e"
      },
      "outputs": [],
      "source": [
        "# these are the values from the linear regression model\n",
        "# compare them to what you got above\n",
        "lin_reg.intercept_, lin_reg.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQfG16MYrzW-"
      },
      "source": [
        "# Linear regression using batch gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CLJrRFErzW-"
      },
      "source": [
        "Just click through this section.\n",
        "\n",
        "The first part shows how to manually do gradient descent. The formula for the gradients has been calculated by hand and we just plug in `X` and `theta`.\n",
        "\n",
        "The second part (function `plot_gradient_descent`) does the gradient descent and plots the first 10 steps.\n",
        "\n",
        "[Here](https://github.com/Jaewan-Yun/optimizer-visualization) is a nice visualization of different gradient descent methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUZNzC6grzW_"
      },
      "outputs": [],
      "source": [
        "eta = 0.1  # learning rate\n",
        "n_iterations = 1000\n",
        "m = 100\n",
        "\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    theta = theta - eta * gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw5Wt52LrzW_",
        "outputId": "60a25401-3597-405f-d075-47cebbabf872"
      },
      "outputs": [],
      "source": [
        "theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyHa0cgrrzXA"
      },
      "outputs": [],
      "source": [
        "theta_path_bgd = []\n",
        "\n",
        "def plot_gradient_descent(theta, eta, theta_path=None):\n",
        "    m = len(X_b)\n",
        "    plt.plot(X, y, \"b.\")\n",
        "    n_iterations = 1000\n",
        "    X_new = np.array([[0], [2]])\n",
        "    X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
        "    for iteration in range(n_iterations):\n",
        "        if iteration < 10:\n",
        "            y_predict = X_new_b.dot(theta)\n",
        "            style = \"b-\" if iteration > 0 else \"r--\"\n",
        "            plt.plot(X_new, y_predict, style)\n",
        "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)   # this line\n",
        "        theta = theta - eta * gradients   # and this one\n",
        "        if theta_path is not None:\n",
        "            theta_path.append(theta)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.axis([0, 2, -10, 15])\n",
        "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "u9UP42oZrzXA",
        "outputId": "c2965e60-a4d1-4c90-8853-8272f7d30ad9"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "theta_path_bgd = []\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(131); plot_gradient_descent(theta, eta=0.02)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\n",
        "plt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXB606II9NZK"
      },
      "source": [
        "For a learning rate $\\eta$ that is too small we approach the minimum too slowly and for a too large learning rate we jump around the minimum like the purple curve in [this animation](https://github.com/Jaewan-Yun/optimizer-visualization/blob/master/figures/movie9.gif) from [this repo](https://github.com/Jaewan-Yun/optimizer-visualization) on GitHub.\n",
        "Note that this animation does not correspond to our data.\n",
        "![animation](https://github.com/Jaewan-Yun/optimizer-visualization/blob/master/figures/movie9.gif?raw=true) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pPLGUcLDB_w"
      },
      "source": [
        "### Task 3.5 (Bonus)\n",
        "Explain what happens in the two lines of code\n",
        "```python\n",
        "gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)   # this line\n",
        "theta = theta - eta * gradients   # and this one\n",
        "```\n",
        "in the code above. Also explain where the first line comes from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLQeFXCqDkhK"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4SGu63sDuHO"
      },
      "source": [
        "Task 3.5 (bonus) answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a-Q3Y0VDr1P"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA1ZmZKvrzXA"
      },
      "source": [
        "# Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amR_zg9OrzXB"
      },
      "source": [
        "This section shows how one can implement Stochastic Gradient Descent by hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-8RC0w2rzXB"
      },
      "outputs": [],
      "source": [
        "theta_path_sgd = []\n",
        "m = len(X_b)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "mpdXF8RGrzXB",
        "outputId": "c652e0a3-cf4b-4581-c93b-c69bd8f70710"
      },
      "outputs": [],
      "source": [
        "n_epochs = 50\n",
        "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
        "\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "X_new = np.array([[0], [2]])\n",
        "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(m):\n",
        "        if epoch == 0 and i < 20: \n",
        "            y_predict = X_new_b.dot(theta)\n",
        "            style = \"b-\" if i > 0 else \"r--\"\n",
        "            plt.plot(X_new, y_predict, style)\n",
        "        random_index = np.random.randint(m)\n",
        "        xi = X_b[random_index:random_index+1]\n",
        "        yi = y[random_index:random_index+1]\n",
        "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(epoch * m + i)\n",
        "        theta = theta - eta * gradients\n",
        "        theta_path_sgd.append(theta) \n",
        "\n",
        "plt.plot(X, y, \"b.\") \n",
        "plt.xlabel(\"$x_1$\", fontsize=18) \n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([0, 2, 0, 15]) \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB09p83qrzXB",
        "outputId": "635f3d8d-1bdb-4857-997e-6b043b3edb2b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW3PHeIyrzXC"
      },
      "source": [
        "### Task 4\n",
        "\n",
        "- Fit an [SGD Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) `sgd_reg` to `X` and `y` and print out its intercept and coefficients. \n",
        "- Compare the values you find to the ones from the stochastic gradient descent above.\n",
        "\n",
        "Use `max_iter=50, tol=-np.infty, penalty=None, eta0=0.1, random_state=42` as parameters for the SGD Regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-ZAT9jlrzXC"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwvYd3w4FUkC"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7uzxyparzXC"
      },
      "outputs": [],
      "source": [
        "# sgd_reg = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgEgY7NUFfwu"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGYI3I2IrzXC"
      },
      "source": [
        "# Mini-batch gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1C9EIZxrzXD"
      },
      "source": [
        "The following code shows how one could implement mini-batch gradient descent by hand. Can you find the difference to Stochastic gradient descent?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDyGZv2TrzXD"
      },
      "outputs": [],
      "source": [
        "theta_path_mgd = []\n",
        "\n",
        "n_iterations = 50\n",
        "minibatch_size = 20\n",
        "\n",
        "np.random.seed(42)\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "t0, t1 = 200, 1000\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "t = 0\n",
        "for epoch in range(n_iterations):\n",
        "    shuffled_indices = np.random.permutation(m)\n",
        "    X_b_shuffled = X_b[shuffled_indices]\n",
        "    y_shuffled = y[shuffled_indices]\n",
        "    for i in range(0, m, minibatch_size):\n",
        "        t += 1\n",
        "        xi = X_b_shuffled[i:i+minibatch_size]\n",
        "        yi = y_shuffled[i:i+minibatch_size]\n",
        "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(t)\n",
        "        theta = theta - eta * gradients\n",
        "        theta_path_mgd.append(theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN-WG-virzXD",
        "outputId": "566bec79-8eef-4270-9140-6847136fbbc2"
      },
      "outputs": [],
      "source": [
        "theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY6CkmGbrzXD"
      },
      "outputs": [],
      "source": [
        "theta_path_bgd = np.array(theta_path_bgd)\n",
        "theta_path_sgd = np.array(theta_path_sgd)\n",
        "theta_path_mgd = np.array(theta_path_mgd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "bAlRIsEdrzXE",
        "outputId": "4cca54a1-1b63-4aaa-ab5c-cdb4d51c11cd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1, label=\"Stochastic\")\n",
        "plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2, label=\"Mini-batch\")\n",
        "plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3, label=\"Batch\")\n",
        "plt.legend(loc=\"upper left\", fontsize=16)\n",
        "plt.xlabel(r\"$\\theta_0$\", fontsize=20)\n",
        "plt.ylabel(r\"$\\theta_1$   \", fontsize=20, rotation=0)\n",
        "plt.axis([2.5, 4.5, 2.3, 3.9])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvPe8JMrzXE"
      },
      "source": [
        "### Task 5\n",
        "Explain which Linear Regression training algorithm you can use if you have a training set with millions of features? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HJaiUFt7Usg"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWjF5XmDEtQW"
      },
      "source": [
        "Task 5 answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjB4sX9KFlE5"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcV_jYLLrzXF"
      },
      "source": [
        "# Polynomial regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn3PrphGrzXE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random as rnd\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD47wIkqrzXF"
      },
      "source": [
        "With a little trick Linear Regression turns out to be rather powerful also for data that is not linear!\n",
        "The trick is to add \"new features\" to `X`. In our case we need a second feature that is just $x^2$. Then we fit again, but because our feature vector is now $(1, x, x^2)$, the dot product with $(\\Theta_0, \\Theta_1, \\Theta_2)$ gives the form for the parabola (check this if you don't see it!).\n",
        "\n",
        "Here we show an example of how to fit a parabola with Linear Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "W1uyl-1KrzXF",
        "outputId": "2460904b-4160-42be-ff6f-d5219f297c44"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "\n",
        "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
        "X_new_poly = poly_features.transform(X_new)\n",
        "y_new = lin_reg.predict(X_new_poly)\n",
        "\n",
        "for style, width, degree in ((\"g-\", 1, 300), (\"b--\", 2, 2), (\"r-+\", 2, 1)):\n",
        "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    std_scaler = StandardScaler()\n",
        "    lin_reg = LinearRegression()\n",
        "    polynomial_regression = Pipeline([\n",
        "            (\"poly_features\", polybig_features),\n",
        "            (\"std_scaler\", std_scaler),\n",
        "            (\"lin_reg\", lin_reg),\n",
        "        ])\n",
        "    polynomial_regression.fit(X, y)\n",
        "    y_newbig = polynomial_regression.predict(X_new)\n",
        "    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)\n",
        "\n",
        "plt.plot(X, y, \"b.\", linewidth=3)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.title(\"Fitting different polynomials\")\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RX8MFNRH8l5"
      },
      "source": [
        "### Task 6\n",
        "- Which curve (red, blue, green) fits the data (blue dots) the best?\n",
        "- Explain what happens to the green curve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icSe9mOO7W74"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EMq8MKCE7_2"
      },
      "source": [
        "Task 6 answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e21Gcly27RC5"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KOQXfIBrzXF"
      },
      "source": [
        "# Training and Validation Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfE3AUIlrzXF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
        "    train_errors, val_errors = [], []\n",
        "    for m in range(1, len(X_train)):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_val_predict = model.predict(X_val)\n",
        "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
        "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "\n",
        "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
        "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
        "    plt.legend(loc=\"upper right\", fontsize=14)   \n",
        "    plt.xlabel(\"Training set size\", fontsize=14) \n",
        "    plt.ylabel(\"RMSE\", fontsize=14)              "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quH_toCnrzXG"
      },
      "source": [
        "Let's compare different degrees of polynomials for fitting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "ngKw-o-ArzXG",
        "outputId": "881ec78c-b0db-435e-a447-d26ec3eb967a"
      },
      "outputs": [],
      "source": [
        "# Linear Regression (degree 1)\n",
        "lin_reg = LinearRegression()\n",
        "plot_learning_curves(lin_reg, X, y)\n",
        "plt.axis([0, 80, 0, 3])                        \n",
        "plt.show()                                     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "ARFBQLB1rzXG",
        "outputId": "7e786ec5-3dee-4991-e012-be9e8e2f76f7"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Degree 2\n",
        "polynomial_regression = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=2, include_bias=True)),\n",
        "        (\"lin_reg\", LinearRegression()),\n",
        "    ])\n",
        "\n",
        "plot_learning_curves(polynomial_regression, X, y)\n",
        "plt.axis([0, 80, 0, 2])           \n",
        "plt.show()                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "KC9gEVpArzXG",
        "outputId": "78ddd7d2-6510-4ab3-86bf-749e42ffeb48"
      },
      "outputs": [],
      "source": [
        "# Degree 20\n",
        "polynomial_regression = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=30, include_bias=True)),\n",
        "        (\"lin_reg\", LinearRegression()),\n",
        "    ])\n",
        "\n",
        "plot_learning_curves(polynomial_regression, X, y)\n",
        "plt.axis([0, 80, 0, 3])           \n",
        "plt.show()                        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srSUIubHrzXH"
      },
      "source": [
        "### Task 7\n",
        "Look at the three plots above that show the training and validation errors for linear regression, quadratic regression and order 20. Compare them to the plot above called \"Fitting different polynomials\".\n",
        "\n",
        "Describe which one is likely overfitting and which one is likely underfitting and why. \n",
        "\n",
        "Hint: Also take the values of the errors into consideration when comparing the different polynomials and not just the shape of the train and validation loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GjY75AyAWL_"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKPlcI1lI4D5"
      },
      "source": [
        "Task 7 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpOfr3BKAcTy"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcHILVuOrzXH"
      },
      "source": [
        "# Regularized models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c-JnVESrzXH"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "m = 20\n",
        "X = 3 * np.random.rand(m, 1)\n",
        "y = 1 + X*X + 1.5 * np.random.randn(m, 1) / 2\n",
        "X_new = np.linspace(0, 3, 1000).reshape(1000, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epdD6Mv1rzXM"
      },
      "source": [
        "### Task 8\n",
        "Fit (`.fit`) a [ridge linear model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with `alpha=1` and Stochastic Average Gradient descent (`solver=\"sag\"`) solver to `X` and `y`. Check which value it predicts (`.predict`) for `X=[[1.5]]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BToG8SUwrzXM"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VALJg_UBHQr"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M1mEPGfrzXM"
      },
      "outputs": [],
      "source": [
        "# ridge_reg_sag = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRMGJ7M9BKKb"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J_YgMtxrzXN"
      },
      "source": [
        "Now we fit a linear model (left plot) and a degree 15 polynomial model (right plot) with ridge regression for different values of alpha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "wXJOazhMrzXN",
        "outputId": "8ad61273-aeca-40ea-e95e-53d79d076b39"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "def plot_model(model_class, polynomial, alphas, **model_kargs):\n",
        "    for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")):\n",
        "        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()\n",
        "        if polynomial:\n",
        "            model = Pipeline([\n",
        "                    (\"poly_features\", PolynomialFeatures(degree=15, include_bias=False)),\n",
        "                    (\"std_scaler\", StandardScaler()),\n",
        "                    (\"regul_reg\", model),\n",
        "                ])\n",
        "        model.fit(X, y)\n",
        "        y_new_regul = model.predict(X_new)\n",
        "        lw = 2 if alpha > 0 else 1\n",
        "        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r\"$\\alpha = {}$\".format(alpha))\n",
        "    plt.plot(X, y, \"b.\", linewidth=3)\n",
        "    plt.legend(loc=\"upper left\", fontsize=15)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.axis([0, 3, 0, 10])\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(122)\n",
        "plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCDx4VkCJeLz"
      },
      "source": [
        "### Task 9\n",
        "- Describe the effect of the regularization parameter alpha in the plot on the right hand side.\n",
        "- Which curve (blue, green, red) would you expect to have the smallest training loss?\n",
        "- Which curve would you expect to generalize the best to new data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK4kWJQqLHck"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS1HfH6aLIyA"
      },
      "source": [
        "Task 7 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r9Ne4pkLIgi"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYjBx6ourzXN",
        "outputId": "97d951cb-327e-4c39-a467-2bd085a5f763"
      },
      "outputs": [],
      "source": [
        "sgd_reg = SGDRegressor(penalty=\"l2\", max_iter=1000, tol=1e-3, random_state=42)\n",
        "sgd_reg.fit(X, y.ravel())\n",
        "sgd_reg.predict([[1.5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94IZ_L8urzXN"
      },
      "source": [
        "The same, but with Lasso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "rQSD89h_rzXN",
        "outputId": "5e3a4c96-2913-41ca-a9fd-b284735b62ce"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(122)\n",
        "plot_model(Lasso, polynomial=True, alphas=(0, 10**-7, 1), random_state=42)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujSNU_ferzXO"
      },
      "source": [
        "### Task 10\n",
        "Fit a Lasso linear model with `alpha=1`. Check which value it predicts for `X=[[1.5]]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APlgquD2rzXO"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_N97bVrEP1Q"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcInIrjgrzXO",
        "outputId": "a50e1a5f-6c3c-4dac-8be2-23ef56298ab7"
      },
      "outputs": [],
      "source": [
        "# lasso_reg = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxjZzo2YEUEx"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaXbwrujrzXP"
      },
      "source": [
        "## Early stopping example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSNv0btDrzXP"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hYP8Ti5rzXP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "poly_scaler = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
        "        (\"std_scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
        "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
        "\n",
        "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
        "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "for epoch in range(1000):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    val_error = mean_squared_error(y_val, y_val_predict)\n",
        "    if val_error < minimum_val_error:\n",
        "        minimum_val_error = val_error\n",
        "        best_epoch = epoch\n",
        "        best_model = deepcopy(sgd_reg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "pEiOzmTjrzXP",
        "outputId": "57c347ec-9af3-4106-fb2c-d8bbdaa6fdba"
      },
      "outputs": [],
      "source": [
        "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
        "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "n_epochs = 500\n",
        "train_errors, val_errors = [], []\n",
        "for epoch in range(n_epochs):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)\n",
        "    y_train_predict = sgd_reg.predict(X_train_poly_scaled)\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    train_errors.append(mean_squared_error(y_train, y_train_predict))\n",
        "    val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "\n",
        "best_epoch = np.argmin(val_errors)\n",
        "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
        "\n",
        "plt.annotate('Best model',\n",
        "             xy=(best_epoch, best_val_rmse),\n",
        "             xytext=(best_epoch, best_val_rmse + 1),\n",
        "             ha=\"center\",\n",
        "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "             fontsize=16,\n",
        "            )\n",
        "\n",
        "best_val_rmse -= 0.03  # just to make the graph look better\n",
        "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
        "plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation set\")\n",
        "plt.plot(np.sqrt(train_errors), \"r--\", linewidth=2, label=\"Training set\")\n",
        "plt.legend(loc=\"upper right\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.ylabel(\"RMSE\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLty71JWrzXQ"
      },
      "source": [
        "### Task 11\n",
        "Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on?\n",
        "What would be options for fixing this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBxdA9gcEjYo"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiQ8itC_LsLC"
      },
      "source": [
        "Task 11 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXsEtJj3Egno"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okk_0qAcrzXQ"
      },
      "source": [
        "### Task 12\n",
        "We want to find at which exact epoch our model is the best.\n",
        "For that we go through 1000 \"epochs\".\n",
        "\n",
        "We have already prepared part of the code for this task. You only have to fill in the last part which should do the following:\n",
        "\n",
        "- if the validation error (of the current epoch) is smaller than the smalles validation error until now (`minimum_val_error`), then:\n",
        "    - set the `minimum_val_error` to be the current validation error\n",
        "    - set the `best_epoch` to be the current epoch\n",
        "    - set the `best_model` to be the current model (`clone(sgd_reg)`)\n",
        "- else: next epoch (you can also just leave out `else`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGsvUE8JrzXQ",
        "outputId": "be268424-dd15-4215-b44d-c4ecb1770ac2"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import clone\n",
        "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True, penalty=None,\n",
        "                       learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "for epoch in range(1000):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    val_error = mean_squared_error(y_val, y_val_predict)\n",
        "    # ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below\n",
        "\n",
        "    # ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this\n",
        "    \n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drZACu6vMYVz"
      },
      "source": [
        "Task 12.5 bonus question: Why do we need `clone(sgd_reg)` here and not just `sgd_reg`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHpMfph7M0-c"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImSMkULxMgP9"
      },
      "source": [
        "Task 12.5 (bonus) answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO-6REp_M3ul"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqqN4XQ5rzXQ"
      },
      "source": [
        "# Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "cgmxSMiprzXR",
        "outputId": "4ce93f92-a35d-49e7-ee4b-fac392c1cf4e"
      },
      "outputs": [],
      "source": [
        "t = np.linspace(-10, 10, 100)\n",
        "sig = 1 / (1 + np.exp(-t))\n",
        "plt.figure(figsize=(9, 3))\n",
        "plt.plot([-10, 10], [0, 0], \"k-\")\n",
        "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
        "plt.plot([-10, 10], [1, 1], \"k:\")\n",
        "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
        "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.legend(loc=\"upper left\", fontsize=20)\n",
        "plt.axis([-10, 10, -0.1, 1.1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q175BB8YM_4e"
      },
      "source": [
        "From here on we will work with the [Iris Flower Data Set](https://en.wikipedia.org/wiki/Iris_flower_data_set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCGkSSy2rzXR",
        "outputId": "8fad1df4-ebbc-4a84-e8bf-41832eda8d81"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "list(iris.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NYUu3OerzXR",
        "outputId": "bcf0cfd3-23b4-4174-a4b9-fc1db526356f"
      },
      "outputs": [],
      "source": [
        "print(iris.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AGHNbX4OHWk"
      },
      "source": [
        "Let's only use one feature: The petal width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJnA7NulrzXR"
      },
      "outputs": [],
      "source": [
        "X = iris[\"data\"][:, 3:]  # petal width only\n",
        "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris virginica, else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUEwUjZerzXR",
        "outputId": "9b6f91bf-e4ac-401a-e013-57538d82ca4f"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "log_reg.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "AbwNtyS6rzXS",
        "outputId": "c3a2159a-1304-4e97-815d-82805cf32f82"
      },
      "outputs": [],
      "source": [
        "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(X[y==0], y[y==0], \"bs\")\n",
        "plt.plot(X[y==1], y[y==1], \"g^\")\n",
        "plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\n",
        "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\n",
        "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")\n",
        "plt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
        "plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
        "plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\n",
        "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
        "plt.ylabel(\"Probability\", fontsize=14)\n",
        "plt.legend(loc=\"center left\", fontsize=14)\n",
        "plt.axis([0, 3, -0.02, 1.02])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNS8yXYRrzXS",
        "outputId": "8f6f94ed-d6d4-4835-a04f-5f90fef2cb9d"
      },
      "outputs": [],
      "source": [
        "decision_boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHnfuLBlOWj_"
      },
      "source": [
        "The decision boundary is at Petal width = 1.66cm.\n",
        "Let's see what we get if we predict the class of an instance slightly to the left (1.5) and one slightly to the right (1.7)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy-BKAq2NZAK",
        "outputId": "6770ac24-ee3d-473f-dbf3-cb9f1eec47c7"
      },
      "outputs": [],
      "source": [
        "# if you are motivated: Try to understand this output\n",
        "log_reg.predict_proba([[1.7], [1.5]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5I1RefjrzXS",
        "outputId": "5c0c8cf1-e162-4d38-fe00-5d1cb5a7bbb1"
      },
      "outputs": [],
      "source": [
        "log_reg.predict([[1.7], [1.5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MobMBsLOvGb"
      },
      "source": [
        "### Task 13\n",
        "What is the class prediction (iris virginica or not iris virginica?) for petal width = 1.7 and what is the prediction for petal width = 1.5? (easy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ola1BG00PINb"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxOueRiLPI6p"
      },
      "source": [
        "Task 13 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abO95ce6PKrO"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkKGQwlMPRM5"
      },
      "source": [
        "Now let's use two features for predicting the class:\n",
        "The petal length and the petal width.\n",
        "We also use all classes this time and not just two like above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extension to 2 dimensions\n",
        "There are no tasks for this part. You can try to understand what happens or skip if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo0ZrnDKrzXS",
        "outputId": "17a30449-6022-40fe-cdd2-7487bf10031b"
      },
      "outputs": [],
      "source": [
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = iris[\"target\"]\n",
        "\n",
        "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\n",
        "softmax_reg.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "augxRMiGrzXS",
        "outputId": "ff5494dd-a79f-47e1-b515-8e39951165f7"
      },
      "outputs": [],
      "source": [
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
        "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "\n",
        "\n",
        "y_proba = softmax_reg.predict_proba(X_new)\n",
        "y_predict = softmax_reg.predict(X_new)\n",
        "\n",
        "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.legend(loc=\"center left\", fontsize=14)\n",
        "plt.axis([0, 7, 0, 3.5])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEL3UyOgrzXT",
        "outputId": "9931ca81-378e-47a4-cef7-8fbccb873b9a"
      },
      "outputs": [],
      "source": [
        "softmax_reg.predict([[5, 2]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYq9mMx1rzXT",
        "outputId": "8ee316d9-e975-45d2-aef1-a7e3e303ba8f"
      },
      "outputs": [],
      "source": [
        "softmax_reg.predict_proba([[5, 2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## General Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBGj1xiqrzXT"
      },
      "source": [
        "### Task 14\n",
        "Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnmNY_mlQJ_y"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpat0HnMrzXT"
      },
      "source": [
        "Task 14 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW6VxC40QMZo"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7liesyzIrzXT"
      },
      "source": [
        "### Task 15\n",
        "Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGyb_lSkQPZa"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRt4Sv5JrzXT"
      },
      "source": [
        "Task 15 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC9eg3tOQOD2"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "02_Training_linear_models_solution.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
